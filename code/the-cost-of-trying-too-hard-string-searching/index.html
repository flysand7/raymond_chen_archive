<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">The cost of trying too hard: String searching</h1>  <!-- .entry-meta -->

<p>There are many algorithms for fast string searching, but the running of a string search is inherently <i>O</i>(<i>n</i>), where <i>n</i> is the length of the string being searched: If <i>m</i> is the length of the string being searched for (which I will call the “target string”), then any algorithm that accesses fewer than <i>n</i>/<i>m</i> elements of the string being searched will have a gap of <i>m</i> unaccessed elements, which is enough room to hide the target string.
 More advanced string searching algorithms can take advantage of characteristics of the target string, but in the general case, where the target string is of moderate size and is not pathological, all that the fancy search algorithms give you over the naive search algorithm is a somewhat smaller multiplicative constant.
 In the overwhelming majority of cases, then, a naive search algorithm is adequate. As long as you’re searching for normal strings and not edge cases like “Find <code>aaaaaaaaaaaaaaab</code> in the string <code>aaaaaaaaaaaaaabaaaaaaaaaaaaaaab</code>“. If you have a self-similar target string, the running time of a naive search is O(<i>mn</i>) where <i>m</i> is the length of the target string. The effort in the advanced searching algorithms goes towards diminishing the effect of <i>m</i>, but pay for it by requiring preliminary analysis of the target string. If your searches are for “relatively short” “normal” target strings, then the benefit of this analysis doesn’t merit the cost.</p>
<p> That’s why nearly all library functions that do string searching use the naive algorithm. The naive algorithm is the correct algorithm over 99% of the time. </p>


</body>