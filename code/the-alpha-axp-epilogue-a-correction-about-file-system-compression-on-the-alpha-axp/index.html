<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">The Alpha AXP, epilogue: A correction about file system compression on the Alpha AXP</h1>  <!-- .entry-meta -->

<p><a href="https://blogs.msdn.microsoft.com/oldnewthing/20161101-00/?p=94615">Some time ago</a>, I claimed that Windows file compression had to be dumbed down in order for the Alpha AXP to hit its performance targets. </p>
<p>I have since been informed by somebody who worked on file system compression for the Windows NT that information was <a href="https://www.vocabulary.com/articles/evasive/the-euphemisms-of-the-decade/">badly sourced</a>. (My source was somebody who worked on real-time file compression, but not specifically on the Windows NT version.) </p>
<p>This is a bit of a futile correction because the wrong information has already traveled around the world <sup>[<a href="http://quoteinvestigator.com/2014/07/13/truth/">citation needed</a>]</sup>, <a href="http://www.theregister.co.uk/2016/11/02/ghost_of_dec_alpha_sees_microsoft_dumb_down_windows_file_compression/">posted some selfies to Instagram</a>, and <a href="https://news.ycombinator.com/item?id=12846104">renewed its passport</a>. </p>
<p>Windows NT file compression worked just fine on the Alpha AXP. It probably got a lot of help from its <a href="https://blogs.msdn.microsoft.com/oldnewthing/20170807-00/?p=96766">abundance of registers</a> its ability to <a href="https://blogs.msdn.microsoft.com/oldnewthing/20170808-00/?p=96775">perform 64-bit calculations natively</a>. </p>
<p>We <a href="http://web.archive.org/web/20170829062643/https://www.poynter.org/tag/regret-the-error">regret the error</a>. </p>
<p><b>Bonus chatter</b>: Real-time file compression is a tricky balancing act. </p>
<p>Compression unit: If you choose a small compression unit, then you don’t get to take advantage of as many compression opportunities. But if you choose a large compression unit, then reads become more inefficient in the case where you needed only a few bytes out of the large unit, because you had to read the entire unit and decompress it, only to get a few bytes. Updates also become more expensive the larger the compression unit because you have to read the entire unit, update the bytes, compress the whole unit, and then write the results back out. (Possibly to a new location if the new data did not compress as well as the original data.) Larger compression units also tend to require more memory for auxiliary data structures in the compression algorithm. </p>
<p>Compression algorithm: Fancier algorithms will give you better compression, but cost you in additional CPU time and memory. </p>
<p>What makes twiddling the knobs particularly difficult is that the effects of the knobs aren’t even monotonic! As you make the compression algorithm fancier and fancier, you may find at first that things get slower and slower, but when the compression ratio reaches a certain level, then you find that the reduction in I/O starts to dominate the extra costs in CPU and memory, and you start winning again. This crossover point can vary from machine to machine because it is dependent upon the characteristics of the hard drive as well as those of the CPU. A high-powered CPU on a slow hard drive is more likely to see a net benefit, whereas a low-powered CPU may never reach the breakeven point. </p>


</body>