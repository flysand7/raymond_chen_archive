<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">Loading the dictionary, part 2:  Character conversion</h1>  <!-- .entry-meta -->

<p>
When you want to optimize a program, you first need to know
where the time is being spent.
There’s no point optimizing a function that isn’t actually
responsible for your poor performance.
For example, if a particular function is responsible for 2% of
your CPU time, then even if you optimized it down to infinite speed,
your program would speed up at best by only little over 2%.
In the comments to yesterday’s entry, several people put forth
suggestions as to how the program could be optimized,
in the process quite amply demonstrating this principle.
None of the people who made suggestions actually investigated
the program to see where the glaring bottleneck was.
</p>
<p>
(<a href="http://blogs.msdn.com/ricom">Rico Mariani</a> points out that you also
need to take performance in account when doing high level designs,
choosing algorithms and data structures that are suitable for
the level of performance you need.
If profiling reveals that a fundamental design decision
is responsible for a performance bottleneck, you’re in big trouble.
You will see this sort of performance-guided design as the program develops.
And you should check out
<a href="http://blogs.msdn.com/ricom/archive/2005/05/10/416151.aspx">
Performance Quiz #6</a>
which starts with the very program we’re developing here.)
</p>
<p>
Upon profiling our dictionary-loader, I discovered that
80% of the CPU time was spent in <code>getline</code>.
Clearly this is where the focus needs to be.
Everything else is just noise.
</p>
<p>
Digging a little deeper, it turns out that
29% of the CPU time was spent by <code>getline</code> doing
character set conversion in <code>codecvt::do_in</code>.
Some debugging revealed that <code>codecvt::do_in</code>
was being called millions of times, each time converting
just one or two characters.  In fact, for each character
in the file, <code>codecvt::do_in</code> was called once
and sometimes twice!
</p>
<p>
Let’s get rid of the piecemeal character set conversion and
instead convert entire lines at a time.
</p>
<pre>
Dictionary::Dictionary()
{
 std::<font color="blue">ifstream</font> src;
 <font color="blue">typedef std::codecvt&lt;wchar_t, char, mbstate_t&gt; widecvt;
 std::locale l(".950");
 const widecvt&amp; cvt = _USE(l, widecvt); // use_facet&lt;widecvt&gt;(l);</font>
 src.open("cedict.b5");
 <font color="blue">string</font> s;
 while (getline(src, s)) {
  if (s.length() &gt; 0 &amp;&amp; s[0] != L'#') {
   <font color="blue">wchar_t* buf = new wchar_t[s.length()];
   mbstate_t state = 0;
   char* nextsrc;
   wchar_t* nextto;
   if (cvt.in(state, s.data(), s.data() + s.length(), nextsrc,
                   buf, buf + s.length(), nextto) == widecvt::ok) {
    wstring line(buf, nextto - buf);</font>
    DictionaryEntry de;
    if (de.Parse(<font color="blue">line</font>)) {
     v.push_back(de);
    }
   <font color="blue">}
   delete[] buf;</font>
  }
 }
}
</pre>
<p>
Instead of using a <code>wifstream</code>,
we just use a non-Unicode <code>ifstream</code>
and convert each line to Unicode manually.
Doing it a line at a time rather than a character at a time,
we hope, will be more efficient.
</p>
<p>
We ask code page 950 for a converter, which we call <code>cvt</code>.
Notice that the Microsoft C++ compiler requires you to use
the strange <code>_USE</code> macro instead of the more traditional
<code>use_facet</code>.
</p>
<p>
For each line that isn’t a comment, we convert it to Unicode.
Our lives are complicated by the fact that <code>codecvt::in</code>
requires pointers to elements rather than iterators, which means
that we can’t use a <code>wstring</code> or a <code>vector</code>;
we need a plain boring <code>wchar_t[]</code> array.
(Notice that we can cheat on the “from” buffer and use the
<code>string::data()</code> function to get at a read-only
array representation of the string.)
If the conversion succeeds, we convert the array into a proper
string and continue as before.
</p>
<p>
With this tweak, the program now loads the dictionary in 1120ms
(or 1180ms if you include the time it takes to destroy the
dictionary).  That’s nearly twice as fast as the previous version.</p>
<p>
You might think that we could avoid redundant allocations
by caching the temporary conversion buffer between lines.
I tried that, and surprisingly, it actually slowed the program
down by 10ms.
Such is
<a href="http://blogs.msdn.com/oldnewthing/archive/2004/12/16/317157.aspx">
the counter-intuitive world of optimization</a>.
That’s why it’s important to identify your bottlenecks via
measurement instead of just guessing at them.</p>


</body>