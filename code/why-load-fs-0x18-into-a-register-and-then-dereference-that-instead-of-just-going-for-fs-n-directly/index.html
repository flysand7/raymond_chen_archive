<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">Why load fs:[0x18] into a register and then dereference that, instead of just going for fs:[n] directly?</h1>  <!-- .entry-meta -->

<p>For the purpose of this discussion, I’m going to assume you are familiar with the x86 32-bit and 64-bit instruction set architectures.</p>
<p>In Windows on x86, a pointer to per-thread information is kept in the <code>fs</code> register (for x86-32) or the <code>gs</code> register (for x86-64). If you disassemble through the kernel, you’ll see that accesses to the per-thread information usually goes through two steps:</p>
<pre>    mov     eax, dword ptr fs:[0x00000018]
    mov     eax, dword ptr [eax+n]
</pre>
<p>Why do it this way when you could combine it into one?</p>
<pre>    mov     eax, dword ptr fs:[n]
</pre>
<p>Access to the per-thread data is abstracted into the helper function <a href="https://docs.microsoft.com/en-us/windows/win32/api/winnt/nf-winnt-ntcurrentteb"> <code>NtCurrentTeb</code></a>, and the definition of that function changes based on the processor architecture.</p>
<pre>// x86-32
return (struct _TEB *) (ULONG_PTR) __readfsdword (0x18);

// x86-64
return (struct _TEB *)__readgsqword(FIELD_OFFSET(NT_TIB, Self));
</pre>
<p>One option for optimizing access to per-thread data is to create a custom accessor for each thing you need.</p>
<pre>inline DWORD GetLastErrorFromTEB()
{
#if defined(_M_IX86)
    return __readfsdword(FIELD_OFFSET(TEB, LastErrorCode));
#elif defined(_M_AMD64)
    return __readgsdword(FIELD_OFFSET(TEB, LastErrorCode));
#else
    ... other architectures here ...
#endif
}
</pre>
<p>This gets rather unwieldy as the number of fields in the <code>TEB</code> increases, since each one needs its own custom accessor. So maybe you abstract it into a macro.</p>
<pre>#if defined(_M_IX86)
#define GetDwordFieldFromTEB(Field) \
    __readfsdword(FIELD_OFFSET(TEB, Field))
#define GetPointerFieldFromTEB(Field) \
    __readfsdword(FIELD_OFFSET(TEB, Field))
#elif defined(_M_AMD64)
#define GetDwordFieldFromTEB(Field) \
    __readgsdword(FIELD_OFFSET(TEB, Field))
#define GetPointerFieldFromTEB(Field) \
    __readgsqword(FIELD_OFFSET(TEB, Field))
#else
    ... other architectures here ...
#endif

DWORD GetLastError()
{
    return GetDwordFieldFromTEB(LastErrorCode);
}

PEB GetProcessEnvironmentBlock()
{
    return (PEB)GetPointerFieldFromTEB(Peb);
}
</pre>
<p>Another option is to teach the compiler’s peephole optimizer that, if compiling in Windows ABI mode, it can fold the instruction sequence, provided the first <code>fs</code> access is to exactly the value <code>0x18</code>. This would be a very special compiler optimization that would kick in only for a limited audience. While it’s true that <a href="https://docs.microsoft.com/en-us/previous-versions/technet-magazine/cc742531(v=msdn.10)"> the compiler team has been known to produce custom versions of the compiler for one-off situations</a>, the savings for this particular micro-optimization would be, if you’re lucky, a few thousands of instructions. That’s a lot of work to save a few dozen kilobytes.</p>
<p>Furthermore, switching over could end up being worse: The offset field of an absolute selector-relative load is a 32-bit field, whereas the offset when applied to a general-purpose register can be made as small as eight bits. There is typically a penalty for accessing memory through a segment register whose base is not zero.¹ Furthermore, you have to pay for the prefix byte, and are probably taking the processor down an execution path that is not heavily optimized. If you are going to access per-thread data more than once in a function, you may very well have been better off just caching the result in a general-purpose register so you could use the smaller (and probably more efficient) flat addressing mode instead.</p>
<p>Even if all the calculations show that accessing the per-thread data from scratch is better than caching it in a general-purpose register (say, because the x86-32 is so register-starved that freeing up even one register is a big deal), you have to redo the cost/benefit calculations for the other architectures.</p>
<pre>// arm
return (struct _TEB *)(ULONG_PTR)_MoveFromCoprocessor(CP15_TPIDRURW);

// arm64
return (struct _TEB *)__getReg(18); // register x18

// alpha
return (struct _TEB *)_rdteb(); // PAL instruction

// ia64
return (struct _TEB *)_rdtebex(); // register r13

// MIPS
return (struct _TEB *)((PCR *)0x7ffff000)-&gt;Teb;

// PowerPC
return (struct _TEB *)__gregister_get(13); // register r13
</pre>
<p>These architectures break down into two categories: Those for which the offset from the TEB register can be folded into the instruction, and those for which it cannot.</p>
<table border="1" cellpadding="3" cellspacing="0" class="cp3" style="border-collapse: collapse;">
<tbody>
<tr>
<th>Architecture</th>
<th>Can fold?</th>
<th>Notes</th>
</tr>
<tr>
<td>x86-32</td>
<td>Yes</td>
<td><code>mov eax, fs:[n]</code></td>
</tr>
<tr>
<td>x86-64</td>
<td>Yes</td>
<td><code>mov rax, gs:[n]</code></td>
</tr>
<tr>
<td>arm</td>
<td>No</td>
<td>Cannot use offset load from coprocessor register</td>
</tr>
<tr>
<td>arm64</td>
<td>Yes</td>
<td><code>ldr x0, [x18, #n]</code></td>
</tr>
<tr>
<td>alpha</td>
<td>No</td>
<td>Address returned by dedicated instruction</td>
</tr>
<tr>
<td>ia64</td>
<td>Yes</td>
<td>Can calculate effective address directly from r13</td>
</tr>
<tr>
<td>MIPS</td>
<td>No</td>
<td>No absolute addressing mode</td>
</tr>
<tr>
<td>PowerPC</td>
<td>Yes</td>
<td><code>lwz r0, n(r13)</code></td>
</tr>
</tbody>
</table>
<p>For the architectures that don’t support folding, there’s no benefit to the optimization. And fetching the address from scratch is a pessimization on Alpha, since the call to get the address of per-thread data is a system call.</p>
<p>In order to benefit from this optimization on processors where it’s helpful, without hurting the ones where it’s harmful, you may end up having to write two versions of every function: One which gets the per-thread pointer from scratch every time, and one which caches it. The compiler cannot do this transformation for you because it doesn’t know whether the per-thread data can safely be cached: If there is a fiber switch, then the per-thread data cannot be cached since the fiber might be resuming on a different thread!</p>
<p>Given that this optimization may not actually be beneficial, and even if it is, the benefit is slight, and even that slight benefit comes at a cost to other architectures, you’re probably better off not bothering with this micro-optimization and just writing portable code.</p>
<p>¹ <a href="https://www.intel.com/content/dam/doc/manual/64-ia-32-architectures-optimization-manual.pdf"> <i>Intel 64 and IA-32 Architectures Optimization Reference Manual</i></a>, April 2012.</p>
<ul>
<li>Chapter 2.1 <i>Intel Microarchitecture Code Name Sandy Bridge</i>, Section 2.1.5.2 <i>L1 DCache</i>: “If segment base is not zero, load latency increases.”</li>
<li>Chapter 13 <i>Intel Atom Microarchitecture and Software Optimization</i>, Section 13.3.3.3 <i>Segment Base</i>: “Non-zero segment base will cause load and store operations to experience a delay.”</li>
</ul>
<p>Fog, Agner. <a href="https://agner.org/optimize/microarchitecture.pdf"> The microarchitecture of Intel, AMD, and VIA CPUs</a>, August 17, 2021.</p>
<ul>
<li>Chapter 19 <i>AMD K8 and K10 pipeline</i>: “The time it takes to calculate an address and read from that address in the level-1 cache is 3 clock cycles if the segment base is zero and 4 clock cycles if the segment base is nonzero, according to my measurements.”</li>
</ul>


</body>