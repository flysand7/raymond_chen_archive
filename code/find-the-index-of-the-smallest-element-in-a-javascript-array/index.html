<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">Find the index of the smallest element in a JavaScript array</h1>  <!-- .entry-meta -->

<p>
Today’s Little Program isn’t even a program.
It’s just a function.
</p>
<p>
The problem statement is as follows:
Given a nonempty JavaScript array of numbers,
find the index of the smallest value.
(If the smallest value appears more than once,
then any such index is acceptable.)
</p>
<p>
One solution is simply to do the operation manually,
simulating how you would perform the operation with
paper and pencil:
You start by saying that the first element is the winner,
and then you go through the rest of the elements.
If the next element is smaller than the one you have,
you declare that element the new provisional winner.
</p>
<pre>
function indexOfSmallest(a) {
 var lowest = 0;
 for (var i = 1; i &lt; a.length; i++) {
  if (a[i] &lt; a[lowest]) lowest = i;
 }
 return lowest;
}
</pre>
<p>
Another solution is to use the <code>reduce</code> intrinsic
to run the loop, so you merely have to provide
the business logic of the
initial guess
and the <code>if</code> statement.
</p>
<pre>
function indexOfSmallest(a) {
 return a.reduce(function(lowest, next, index) {
                   return next &lt; a[lowest] : index ? lowest; },
                 0);
}
</pre>
<p>
A third solution is to use JavaScript intrinsics to
find the smallest element and then convert
the element to its index.
</p>
<pre>
function indexOfSmallest(a) {
 return a.indexOf(Math.min.apply(Math, a));
}
</pre>
<p>
Which one is fastest?
</p>
<p>
Okay, well, first, before you decide which one is fastest,
you need to make sure they are all correct.
One thing you discover is that the <code>min/indexOf</code>
technique fails once the array gets really, really large,
or at least it does in Internet Explorer and Firefox.
(In my case, Internet Explorer and Firefox gave up at around
250,000 and 500,000 elements, respectively.)
That’s because you start hitting engine limits on the number
of parameters you can pass to a single function.
Invoking <code>apply</code> on an array of 250,000 elements
is the equivalent of calling <code>min</code> with 250,000
function parameters.
</p>
<p>
So we’ll limit ourselves to arrays of length at most 250,000.
</p>
<p style="height: 10em">
Before I share the results, I want you to guess which
algorithm you think will be the fastest
and which will be the slowest.
</p>
<p style="height: 10em">
Still waiting.
</p>
<p>
I expected the manual version to come in last place,
because, after all, it’s doing everything manually.
I expected the reduce version to be slightly faster,
because it offloads some of the work into an intrinsic
(though the function call overhead may have negated
any of that improvement).
I expected the min/indexOf version to be fastest
because nearly all the work is being done in intrinsics,
and the cost of making two passes over the data
would be made up by the improved performance of the intrinsics.
</p>
<p>
Here are the timings of the three versions with arrays of
different size, running on random data.
I’ve normalized run times so that the results are independent
of CPU speed.
</p>
<table border="1" cellpadding="3" style="border-collapse: collapse">
<caption>
<font size="+1"><b>Relative running time per array element</b></font>
</caption>
<tr>
<th>Elements</th>
<th>manual</th>
<th>reduce</th>
<th>min/indexOf</th>
</tr>
<tr>
<th colspan="4">Internet Explorer 9</th>
</tr>
<tr>
<td align="right">100,000</td>
<td align="right">1.000</td>
<td align="right">2.155</td>
<td align="right">2.739</td>
</tr>
<tr>
<td align="right">200,000</td>
<td align="right">1.014</td>
<td align="right">2.324</td>
<td align="right">3.099</td>
</tr>
<tr>
<td align="right">250,000</td>
<td align="right">1.023</td>
<td align="right">2.200</td>
<td align="right">2.330</td>
</tr>
<tr>
<th colspan="4">Internet Explorer 10</th>
</tr>
<tr>
<td align="right">100,000</td>
<td align="right">1.000</td>
<td align="right">4.057</td>
<td align="right">4.302</td>
</tr>
<tr>
<td align="right">200,000</td>
<td align="right">1.028</td>
<td align="right">4.057</td>
<td align="right">4.642</td>
</tr>
<tr>
<td align="right">250,000</td>
<td align="right">1.019</td>
<td align="right">4.091</td>
<td align="right">4.068</td>
</tr>
</table>
<p>
Are you surprised?
I sure was!
</p>
<p>
Not only did I have it completely backwards,
but the margin of victory for the manual version was way
beyond what I imagined.
</p>
<p>
(This shows that the only way to know
your program’s performance characteristics for sure
is to <i>sit down and measure it</i>.)
</p>
<p>
What I think is going on is that the JavaScript optimizer
can do a really good job of optimizing the manual code
since it is very simple.
There are no function calls, the loop body is just one line,
it’s all right out there in the open.
The versions that use intrinsics end up hiding some of the
information from the optimizer.
(After all, the optimizer cannot predict ahead of time whether
somebody has overridden the default implementation of
<code>Array.prototype.reduce</code> or
<code>Math.prototype.min</code>,
so it cannot blindly inline the calls.)
The result is that the manual version can run over twice
as fast on IE9 and over four times as fast on IE10.
</p>
<p>
I got it wrong because I thought of JavaScript too much like
an interpreted language.
In a purely interpreted language,
the overhead of the interpreter is roughly proportional
to the number of things you ask it to do,
as opposed to how hard it was to do any of those things.
It’s like a fixed service fee imposed on every transaction,
regardless of whether the transaction was for $100 or 50 cents.
You therefore try to make one big purchase (call a complex intrinsic)
instead of a lot of small ones (read an array element,
compare two values, increment a variable, copy one variable to another).
</p>
<p>
<b>Bonus chatter</b>:
I ran the test on Firefox, too,
because I happened to have it handy.
</p>
<table border="1" cellpadding="3" style="border-collapse: collapse">
<caption>
<font size="+1"><b>Relative running time per array element</b></font>
</caption>
<tr>
<th>Elements</th>
<th>manual</th>
<th>reduce</th>
<th>min/indexOf</th>
</tr>
<tr>
<th colspan="4">Firefox 16</th>
</tr>
<tr>
<td align="right">100,000</td>
<td align="right">1.000</td>
<td align="right">21.598</td>
<td align="right">3.958</td>
</tr>
<tr>
<td align="right">200,000</td>
<td align="right">0.848</td>
<td align="right">21.701</td>
<td align="right">2.515</td>
</tr>
<tr>
<td align="right">250,000</td>
<td align="right">0.839</td>
<td align="right">21.788</td>
<td align="right">2.090</td>
</tr>
</table>
<p>
The same data collected on Firefox 16
(which sounds ridiculously old because Firefox
will be on version 523 by the time this
article reaches the head of the queue)
shows a different profile,
although the winner is the same.
The manual loop and the min/indexOf get more efficient
as the array size increases.
This suggests that there is fixed overhead that becomes
gradually less significant as you increase the size of the
data set.
</p>
<p>
One thing that jumps out is that the reduce method way
underperforms the others.
My guess is that setting up the function call
(in order to transition between the intrinsic and the script)
is very expensive,
and that implementors of the JavaScript engines haven’t
spent any time optimizing this case because
<code>reduce</code> is not used much in real-world code.
</p>
<p>
<b>Update</b>:
I exaggerated my naïveté to make for a better
narrative.
As I point out in
<a href="http://my.safaribooksonline.com/book/programming/microsoft-windows/0321440307/preface/pref02">
the preface</a>
to
<a href="http://blogs.msdn.com/oldnewthing/archive/2006/12/07/1233002.aspx">
my book</a>,
my stories may not be completely true,
but they are true enough.
Of course I know that JavaScript is jitted nowadays,
and that changes the calculus.
(Also, the hidden array copy.)</p>


</body>