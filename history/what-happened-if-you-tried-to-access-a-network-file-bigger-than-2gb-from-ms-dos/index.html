<body style=max-width:80ch;margin:auto>
<h1 class="entry-title" style="margin-bottom:15px;">What happened if you tried to access a network file bigger than 2GB from MS-DOS?</h1>  <!-- .entry-meta -->

<p>One of my friends is into retrocomputing, and he wondered what happened on MS-DOS if you asked it to access a file on a network share that was bigger than what FAT16 could express.</p>
<p>My friend was under the mistaken impression that when MS-DOS accessed a network resource, it was the sector access that was remoted. Under this model, MS-DOS would still open the boot sector, look for the FAT, parse it, then calculate where the directories were, read them directly from the network hard drive, and write raw data directly to the network hard drive.</p>
<p>This is not how it works.</p>
<p>For one thing, if it worked like that, then if two clients both accessed a network hard drive, they would corrupt each other. Each one has its own locally-cached copy of the FAT, and when it came time to allocate a new cluster, each one would pick a cluster (probably the same one), and assign that cluster to the new data.</p>
<p>What actually happens is that <a href="https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-cifs/d416ff7c-c536-406e-a951-4f04b2fd1d2b?fbclid=IwAR2fL6VpPMJMiAL4q0pRDDsBfWHc_ztTo-3g24b5FPXQJLQyIiv0IRD9nI0"> the file system operations themselves are sent remotely</a>, rather than the low-level disk operations. You would send send requests to the server like “Please open a file called <code>AWESOME.TXT</code> in write mode” or “Please tell me how big the file <code>README.DOC</code> is.” The remote server translated these requests into its own native file system, performed the operation, and sent the results back to the MS-DOS client.</p>
<p>The server need not be running a FAT file system. In practice, it was probably running Novell NetWare.</p>
<p>The next question was, “But what happens if the file is bigger than 2GB?”</p>
<p>A file bigger than 2GB? What planet are you from?</p>
<p>We’re talking 1984 here. A 20 <i>megabyte</i> hard drive costed around $1800. To get to 2GB, you’d first have to invent RAID. Then create an array of 100 drives, which would put you at $180,000. Though you could probably get a bulk discount. And you’d have to be able to connect them and power them all. And then to create that file, you’d need to push 2GB of data over a T1 line, which would take about three hours.</p>
<p>My friend explained, “Well, let’s say that the super-huge file is on a supercomputer somewhere. You’re not downloading the file, but rather seeking to selected portions and reading little bits. How would that work for a file bigger than 2GB?”</p>
<p>The response to the request for file attributes had <a href="https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-cifs/847573c9-cbe6-4dcb-a0db-9b5af815759b"> a 32-bit value for the file size</a>. So if your file was 2GB, that would still fit.</p>
<p>Read requests took the form of <a href="https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-cifs/23704aa0-e6d2-4762-8dfd-e8eeaacca71b"> a 32-bit file offset an a 16-bit size</a>. If your file was bigger than 4GB, you would have no way to access any bytes beyond 4GB because it wouldn’t fit in the 32-bit file offset.</p>
<p>Of course, if you’re retrocomputing, then your poor 1984 MS-DOS system is going to be seeing wild and crazy things from the future, like hard drives bigger than 20MB and processors that can count to a billion in less than 55ms. Its brain might explode (or divide by zero). But that’s part of what makes retrocomputing fun, I guess.</p>
<p> </p>


</body>